# Span, linear dependence and linear independence

Recall that a linear combination of vectors
means the result of multiplying each vector by a number (called a coefficient)
and adding the resulting vectors.
For example, $(2,3) + 4(5,6) - 7(8,9)$ is
a linear combination of the vectors $(2,3)$, $(5,6)$ and $(8,9)$,
with coefficients $1$, $4$ and $-7$.
We now look at all linear combinations that can be made from given vectors.

graybox:
    The **span** of vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$
    means the set of all their linear combinations.
    It is denoted with $\span(\vec{v_1},\dots,\vec{v_n})$.

Examples:
<ul><li>
    By combining the vectors $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$,
    we can create any 3D vector $(x,y,z)$,
    because $x(1,0,0) + y(0,1,0) + z(0,0,1) = (x,y,z)$.
    In other words, the span of $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$
    is the set of all 3D vectors.
    We can write this as
    $$
    \span((1,0,0), (0,1,0), (0,0,1)) = \mathbb{R}^3.
    $$
    Here $\mathbb{R}$ means the set of all numbers,
    and $\mathbb{R}^3$ means the set of all sequences of 3 numbers,
    i.e. the set of all 3D vectors.
</li><li>
    The span of only $(1,0,0)$ and $(0,1,0)$ is the set of vectors like
    $$
    x(1,0,0) + y(0,1,0) = (x,y,0),
    $$
    where $x$ and $y$ are any numbers.
    In other words, this is the set of all 3D vectors whose $z$ coordinate is zero.
    This set is known as the xy plane.

asymptote3d:
    unitsize(3cm);
    currentprojection = perspective(camera=(5,-10,6), up=Z);
    axises3d(-2, 2, -2, 2, -0.5, 1.5, xlabel="$x$", ylabel="$y$", zlabel="$z$");
    path rect = scale(3, 3)*shift(-0.5, -0.5)*unitsquare;
    draw(surface(rect), yellow+opacity(0.8));

The span is infinitely big in all directions,
but because an image can't be infinitely big,
I only drew a square-shaped part of the span.

</li><li>
    We can also take the span of only one vector.
    For example, the span of $(1,1)$ is the set of all vectors like $a(1,1)$,
    where $a$ is any number.
    They form a line going through origin
    with [slope](../analytic-plane-geometry/line-eq-slope.html#meaning-of-s-and-h) 1.

asymptote:
    unitsize(1.5cm);
    axises(-3,3,-3,3);
    draw((-3,-3)--(3,3), p=blue, L=Label("$\Span\left(\begin{bmatrix}1\\1\end{bmatrix}\right)$", position=Relative(1), align=SE));
    dot((1,1), L="$(1,1)$", align=cis(2), p=dotpen+deepred);
    dot((2,2), L="$2(1,1)$", align=NW, p=dotpen+deepgreen);
    dot((-0.4,-0.4), L="$-0.4(1,1)$", align=W, p=dotpen+deepgreen);
    dot((-1.5,-1.5), L="$-\frac{3}{2}(1,1)$", align=W, p=dotpen+deepgreen);

</li><li>
    $\span((2,2))$ is the same set as $\span((1,1))$,
    because the vectors go in the same direction,
    and therefore their spans produce the same line.
    In general, the length of the original vectors doesn't affect their span,
    because the vectors get multiplied by all numbers anyway.
</li><li>
    The span of $\blue{(1,1,0)}$ and $\red{(0,0,1)}$
    is a plane.
    On the xy plane, it looks just like the previous $\blue{\span((1,1))}$ example.
    Including $\red{(0,0,1)}$ basically adds a vertical direction to the span.

asymptote3d:
    unitsize(1.7cm);

    currentprojection = perspective(camera=(5,-10,6), up=Z);
    axises3d(-2, 3, -2, 3, -1, 3.5, xlabel="$x$", ylabel="$y$", zlabel="$z$");

    path rect = shift(-sqrt(2), -1)*scale(5*sqrt(2), 4)*unitsquare;
    draw(rotate(45, Z)*rotate(90, X)*surface(rect), yellow+opacity(0.8));
    draw(path3((-1,-1)--(4,4)), p=blue);
    dot((1,1,0), p=dotpen+blue, L="$(1,1,0)$", align=SE);
    dot((3,3,0), p=dotpen+blue, L="$3(1,1,0)$", align=SE);
    dot((0,0,1), p=dotpen+deepred, L="$(0,0,1)$", align=E);
    dot((0,0,2), p=dotpen+deepred, L="$2(0,0,1)$", align=E);
    dot((3,3,2), p=dotpen+deepgreen, L="$3(1,1,0)+2(0,0,1)$", align=E);

</li></ul>

See also [3blue1brown's video](https://www.youtube.com/watch?v=k7RM-ot2NWY)
for better visualizations of spans.


## Linear dependence

We consider what happens when adding a vector $\vec w$
to a span of other vectors $\vec{v_1}$, $\vec{v_2}$ and $\vec{v_3}$.
To make this easier to follow,
we call $\span(\vec{v_1},\vec{v_2},\vec{v_3})$ the old span, and
we call $\span(\vec{v_1},\vec{v_2},\vec{v_3},\vec w)$ the new span.

Because you can choose to not use $\vec w$
by making its coefficient zero,
such as $2\vec{v_1}+4\vec{v_2}-5\vec{v_3}+0\vec w$,
any vector in the old span is also in the new span.
However, the new span can also contain other vectors that aren't in the old span.
To see when that happens, we split this into two cases:
<ul><li>
    Assume that $\blue{\vec w}$ is in the old span.
    Consider any vector of the new span,
    such as $\green{\vec{v_1}+4\vec{v_2} + 7\vec{v_3}} + 6\blue{\vec w}$.
    Because $\blue{\vec w}$ is in the old span,
    we can replace it with a linear combination of other vectors.
    For example, if $\blue{\vec w = 2\vec{v_1} + 3\vec{v_2} - 2\vec{v_3}}$, we get
    $$
    \begin{align}
    \green{\vec{v_1} + 4\vec{v_2} + 7\vec{v_3}} + 6\blue{\vec w}
    &= \green{\vec{v_1} + 4\vec{v_2} + 7\vec{v_3}} + 6\blue{( 2\vec{v_1} + 3\vec{v_2} - 2\vec{v_3})} \\
    &= (\green{1} + 6 \cdot \blue2)\vec{v_1} + (\green{4} + 6 \cdot \blue3)\vec{v_2} + (\green7 + 6 \cdot \blue{(-2)}) \vec{v_3} \\
    &= 13\vec{v_1} + 22\vec{v_2} - 5\vec{v_3},
    \end{align}
    $$
    which is a vector in the old span.
    Because any vector in the new span is also in the old span,
    and any vector in the old span is also in the new span,
    the spans must be the same set.
</li><li>
    Even if $\red{\vec w}$ is not in the old span,
    it is in the new span, because it can be written as
    $$
    \red{\vec w} = 0\vec{v_1} + 0\vec{v_2} + 0\vec{v_3} + 1\red{\vec{w}}.
    $$
</li></ul>

In short, you can't "expand" a span to contain more vectors by using vectors from the span itself,
but it can be done by adding a new vector that isn't already in the span.

graybox:
    Adding a vector to a span
    can either do nothing or make the span bigger,
    depending on whether the vector is already in the span:
    <ul><li>
        If $\blue{\vec w}$ is already in $\span(\vec{v_1},\dots,\vec{v_n})$,
        then $\span(\vec{v_1},\dots,\vec{v_n},\blue{\vec w})=\span(\vec{v_1},\dots,\vec{v_n})$.
    </li><li>
        If $\red{\vec w}$ is not in $\span(\vec{v_1},\dots,\vec{v_n})$,
        then $\span(\vec{v_1},\dots,\vec{v_n},\red{\vec w})$ contains
        all vectors of $\span(\vec{v_1},\dots,\vec{v_n})$
        and other vectors too (such as $\red{\vec w}$).
    </li></ul>

Let's now define a couple new concepts:

graybox:
    We say that vectors $\vec{v_1},\vec{v_2},\dots,\vec{v_n}$
    are **linearly dependent**,
    if one of the vectors is a linear combination of others.

Based on our previous result,
linearly dependent means that the span has an unnecessary vector,
and would be the same with one of the vectors removed.
If this can't be done, i.e. if every vector is needed to get the span,
then we say that the vectors are linearly independent.

graybox:
    We say that vectors $\vec{v_1},\dots,\vec{v_n}$ are **linearly independent**
    if they are not linearly dependent;
    that is, if no vector can be written as a linear combination of others.

Examples:
<ul><li>
    The vectors $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$ are linearly independent.
    For example, you can't write $(0,0,1)$
    as a linear combination of $(1,0,0)$ and $(0,1,0)$,
    because any combination of those vectors is within the xy plane,
    but $(0,0,1)$ isn't.
    This means that all three vectors are needed
    for $\span((1,0,0), (0,1,0), (0,0,1))$ to be
    the set of all 3D vectors.
</li><li>
    The vectors $(1,1)$ and $(2,2)$ are linearly dependent,
    because you can write $(2,2)$ as the linear combination $2(1,1)$.
    Therefore $(2,2)$ is unnecessary in $\span((1,1),(2,2))$,
    and we have
    $$
    \span((1,1),(2,2)) = \span((1,1)).
    $$
    On the other hand, we can also view $(1,1)$ as the unnecessary vector,
    because $(1,1) = \frac{1}{2}(2,2)$, and therefore
    $$
    \span((1,1),(2,2)) = \span((2,2)).
    $$
    Earlier on this page I explained visually why $\span((1,1))=\span((2,2))$,
    and now we found a different way to arrive at the same result.
</li><li>
    Linear dependence means that you can remove some vector without affecting the span.
    This doesn't mean that you can remove any vector you want.
    For example, the vectors $(0,0)$, $(1,0)$ and $(0,1)$
    are linearly dependent,
    and $(0,0)$ can be removed from $\span((0,0),(1,0),(0,1))$ without affecting it, because
    $$
    (0,0) = 0(1,0) + 0(0,1).
    $$
    However, removing $(0,1)$ affects the span, because $\span((0,0),(1,0))$ is the $x$ axis,
    even though $\span((0,0),(1,0),(0,1))=\mathbb{R}^2$ is the set of all 2D vectors.
    Removing $(1,0)$ also affects the span.

asymptote:
    unitsize(3cm);
    axises(-0.5, 2, -0.5, 2);
    dot((0,0), p=dotpen+blue, L="$(0,0)$", align=NW);
    dot((1,0), p=dotpen+deepred, L="$(1,0)$", align=N);
    dot((0,1), p=dotpen+deepred, L="$(0,1)$");

</li></ul>


comment:
    ## Multiplying by a number

    Consider a span that has a vector $\vec v$ and also a multiple $5\vec v$,
    such as
    $$
    \span(\vec u,\vec v,5\vec v,\vec w).
    $$
    On the one hand, $5\vec v = 0\vec u + 5\vec v + 0\vec w$
    is a linear combination of other vectors,
    and therefore doesn't affect the span:
    $$
    \span(\vec u,\vec v,5\vec v,\vec w) = \span(\vec u,\vec v,\vec w).
    $$
    On the other hand, $\vec v = \frac{1}{5}(0\vec u + 5\vec v + 0\vec w)$
    is also a linear combination of other vectors, so
    $$
    \span(\vec u,\vec v,5\vec v,\vec w) = \span(\vec u,5\vec v,\vec w).
    $$
    This means that
    $$
    \span(\vec u,\vec v,\vec w) = \span(\vec u,5\vec v,\vec w).
    $$
    This works the same way with any number of vectors,
    and with any nonzero number instead of 5.
    We needed to divide by 5,
    and $\span((1,0))$ is obviously not same as $\span(0(1,0), (0,1))$, for example.

    graybox:
        The span doesn't change if you multiply one of the vectors by a nonzero number.


## Counting vectors on both sides

Let $\vec u$, $\vec v$ and $\vec w$ be linearly independent vectors.
Consider the equation
$$
a\vec{u}+b\vec{v}+c\vec{w} = x\vec{u}+y\vec{v}+z\vec{w}.
$$
This can be rewritten as
$$
(a-x)\vec{u} = (y-b)\vec{v} + (z-c)\vec{w}.
$$
We must have $a-x = 0$, because otherwise we could divide by $a-x$, and we would get
$$
\vec{u} = \frac{y-b}{a-x}\vec v + \frac{z-c}{a-x}\vec w,
$$
which is impossible, because the vectors are linearly indepependent
and therefore $\vec u$ can't be a linear combination of $\vec v$ and $\vec w$.
Because $a-x=0$, we must have $a=x$,
and we similarly get $b=y$ and $c=z$.
This works the same way with any number ($n$ below) of linearly independent vectors.

graybox:
    Let $\vec{v_1},\dots,\vec{v_n}$ be linearly independent vectors.
    If two of their linear combinations produce the same vector,
    then the coefficients of the two linear combinations must match.
    In other words, if
    $$
    a_1\vec{v_1}+a_2\vec{v_2}+\dots+a_n\vec{v_n} = b_1\vec{v_1}+b_2\vec{v_2}+\dots+b_n\vec{v_n}
    $$
    with some coefficients $a_1,\dots,a_n$ and $b_1,\dots,b_n$,
    then $a_1=b_1$, $a_2=b_2$, and so on.

From now on, I will refer to this as **counting vectors on both sides**,
because we essentially count how many $\vec{v_1}$'s, $\vec{v_2}$'s etc. there is on each side.

Another way to interpret this result is that
there can't be multiple ways to write the same vector
as a linear combination of $\vec{v_1},\dots,\vec{v_n}$,
because any two ways must in fact be the same way,
in the sense that they have the same coefficients.

Counting vectors on both sides doesn't work with linearly dependent vectors.
For example, if $\vec u = 2\vec v + 3\vec w$,
then by counting $\vec u$ on both sides we would get $1 = 0$,
because there's one $\vec u$ on the left side and none on the right side.


## Adding multiples doesn't affect linear dependence

Let $\red{\vec u}$, $\blue{\vec v}$ and $\green{\vec w}$ be linearly independent vectors.
If $\red{\vec u}$, $\blue{\vec v}$ and $\green{\vec w} + 2\blue{\vec v}$ are linearly dependent,
we get a contradiction:
<ul><li>
    If $\red{\vec u}$ was a linear combination of $\blue{\vec v}$ and $\green{\vec w} + 2\blue{\vec v}$,
    it would also be some linear combination of $\blue{\vec v}$ and $\green{\vec w}$.
    This is impossible, because $\red{\vec u}$, $\blue{\vec v}$ and $\green{\vec w}$ are linearly independent.
</li><li>
    If $\green{\vec w} + 2\blue{\vec v}$ was a linear combination of $\red{\vec u}$ and $\blue{\vec v}$,
    it would lead to
    $$
    \green{\vec w} + 2\blue{\vec v} = a\red{\vec u} + b\blue{\vec v},
    $$
    with some numbers $a$ and $b$,
    and now counting the vector $\green{\vec w}$ on both sides (see above) would give $1=0$.
</li><li>
    If $\blue{\vec v}$ was a linear combination of $\red{\vec u}$ and $\green{\vec w} + 2\blue{\vec v}$,
    we would have
    $$
    \blue{\vec v} = a\red{\vec u} + b(\green{\vec w} + 2\blue{\vec v}).
    $$
    We can now count $\green{\vec w}$ on both sides to get $0=b$,
    which leads to $\blue{\vec v} = a\red{\vec u}$.
    This is impossible, because now $\blue{\vec v}$ is a linear combination of $\red{\vec u}$ (and $\green{\vec w}$),
    even though $\red{\vec u}$, $\blue{\vec v}$ and $\green{\vec w}$ are linearly indepepdent.
</li></ul>

So if $\red{\vec u}$, $\blue{\vec v}$ and $\green{\vec w}$ are linearly independent,
then $\red{\vec u}$, $\blue{\vec v}$ and $\green{\vec w} + 2\green{\vec w}$ are also linearly independent:
adding $2\blue{\vec v}$ to $\green{\vec w}$ preserves the linear independence.

The same also works with dependent instead of independent:
if $\red{\vec u}$, $\blue{\vec v}$ and $\green{\vec w}$ are linearly dependent,
then $\red{\vec u}$, $\blue{\vec v}$ and $\green{\vec w} + 2\blue{\vec v}$ are also linearly dependent.
They cannot be independent, because if they were,
then you could add $-2\blue{\vec v}$ to get $\green{\vec w} + 2\blue{\vec v}$ back to just $\green{\vec w}$;
this would lead to $\red{\vec u}$, $\blue{\vec v}$ and $\green{\vec w}$ being linearly independent.

These results work with any number instead of $2$
(in other words, with any multiple of $\blue{\vec v}$ instead of $2\blue{\vec v}$),
and also with any number (zero or more) vectors instead of one vector $\red{\vec u}$.

graybox:
    With two or more vectors,
    adding a multiple of one vector to another doesn't affect linear dependence or independence.


## Checking linear dependence/independence

Using the above result, you can check whether given vectors are linearly dependent or independent
by repeatedly adding multiples of one vector from another
until you arrive at vectors that are obviously dependent or independent,
e.g. they contain mostly zeros.
You can also change the order of the vectors, if you want;
that doesn't affect dependence or independence either.
In this context, it is common to write the vectors in a matrix
so that each row of the matrix represents one vector.

For example, let's check if the vectors $(3, -2, 0)$, $(5, 1, 2)$ and $(-2, 4, 3)$
are linearly dependent or independent.

python:
    from fractions import Fraction as F
    from linalg_utils import MatrixWithRowOperations
    matrix = MatrixWithRowOperations([
        [3, -2, 0],
        [5, 1, 2],
        [-2, 4, 3],
    ])
    matrix.add_multiple(src=0, dest=2, scalar=2)
    matrix.add_multiple(src=0, dest=1, scalar=F(1,2))
    matrix.add_multiple(src=1, dest=2, scalar=F(-3,2))
    matrix.add_multiple(src=2, dest=1, scalar=F(26,23))
    matrix.add_multiple(src=2, dest=0, scalar=F(12,23))
    return "$$" + matrix.get_output() + "$$"

The numbers to multiply by are chosen so that we get as much zeros as possible.
For example, in the first step, I multiplied by 2 because that resulted in
$\green{4} + 2\red{(-2)} = \magenta0$ in the bottom row.

The resulting vectors $\green{(0,-2,0)}$, $\magenta{(0,0,2)}$ and $\blue{(-23/4,0,0)}$ are linearly independent,
because none of them is a linear combination of others:
for example, every linear combination of $\green{(0,-2,0)}$ and $\magenta{(0,0,2)}$
starts with zero, but $\blue{(-23/4,0,0)}$ doesn't.
This means that the original vectors $(3, -2, 0)$, $(5, 1, 2)$ and $(-2, 4, 3)$
are also linearly independent.

As you can see, this easily turns into a mess of fractions.
To avoid it, you can at any time multiply a row by a nonzero number;
this doesn't affect the linear dependence or independence.
For example, when you get a row like $\green{(\frac{13}{2},0,2)}$,
you can multiply it by 2 to get $\blue{(13,0,4)}$ instead.
Here's the same calculation again, but with this trick in use:

python:
    from fractions import Fraction as F
    from linalg_utils import MatrixWithRowOperations
    matrix = MatrixWithRowOperations([
        [3, -2, 0],
        [5, 1, 2],
        [-2, 4, 3],
    ])
    matrix.add_multiple(src=0, dest=2, scalar=2)
    matrix.add_multiple(src=0, dest=1, scalar=F(1,2))
    matrix.multiply_row(1, by=2)
    matrix.add_multiple(src=1, dest=2, scalar=F(-3,4))
    matrix.multiply_row(2, by=F(4,23))
    matrix.add_multiple(src=2, dest=1, scalar=13)
    matrix.add_multiple(src=2, dest=0, scalar=3)
    return "$$" + matrix.get_output() + "$$"

As you can see, there are more steps, but each step is easier to follow.


comment:
    ## Adding multiples doesn't affect span

    Consider the spans
    $$
    \span(\red{\vec u}, \blue{\vec v}) \quad \text{and} \quad
    \span(\red{\vec u}, \green{\vec v + 2\vec u}).
    $$
    Because $\green{\vec v + 2\vec u}$ is a linear combination of $\red{\vec u}$ and $\blue{\vec v}$,
    adding it to their span doesn't change anything:
    $$
    \span(\red{\vec u}, \blue{\vec v}) = \span(\red{\vec u}, \blue{\vec v}, \green{\vec v + 2\vec u}).
    $$
    On the other hand,
    $$
    \blue{\vec v} = \green{\vec v + 2\vec u} - 2\red{\vec u}
    $$
    is a linear combination of $\green{\vec v + 2\vec u}$ and $\red{\vec u}$, so
    $$
    \span(\red{\vec u}, \green{\vec v + 2\vec u}) = \span(\red{\vec u}, \blue{\vec v}, \green{\vec v + 2\vec u}).
    $$
    By putting it all together, we get
    $$
    \span(\red{\vec u}, \blue{\vec v}) =
    \span(\red{\vec u}, \green{\vec v + 2\vec u}).
    $$
    This also works with any number instead of 2, and also with more than two vectors.

    graybox:
        The span doesn't change when adding a multiple of one vector to another.

    Here a multiple of a vector means the vector multiplied by some number.
    For example, $5\red{\vec u}$, $-2\red{\vec u}$ and $0\red{\vec u}$
    are multiples of $\red{\vec u}$.
